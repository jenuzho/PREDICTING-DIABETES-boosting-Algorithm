{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "\n",
                "<br>\n",
                "\n",
                "<br>\n",
                "\n",
                "# ðŸš€ **PREDICTING DIABETES** ðŸš€"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**BOOSTING ALGORITHM (XGBOOST)**\n",
                "\n",
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## **INDEX**\n",
                "\n",
                "- **STEP 1: PROBLEM DEFINITION AND DATA COLLECTION**\n",
                "- **STEP 2: DATA EXPLORATION AND CLEANING**\n",
                "- **STEP 3: UNIVARIATE VARIABLE ANALYSIS**\n",
                "- **STEP 4: MULTIVARIATE VARIABLE ANALYSIS**\n",
                "- **STEP 5: FEATURE ENGINEERING**\n",
                "- **STEP 6: FEATURE SELECTION**\n",
                "- **STEP 7: MACHINE LEARNING**\n",
                "- **STEP 8: CONCLUSIONS**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "\n",
                "### **STEP 1: PROBLEM DEFINITION AND DATA COLLECTION**\n",
                "\n",
                "- 1.1. Problem Definition\n",
                "- 1.2. Library Importing\n",
                "- 1.3. Data Collection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**1.1. PROBLEM DEFINITION**\n",
                "\n",
                "Diabetes is a chronic health condition that affects millions of people worldwide. Early detection and diagnosis of diabetes are crucial for effective management and prevention of complications. In this study, we aim to develop a predictive model that can accurately identify individuals at risk of developing diabetes based on a set of diagnostic measures. By leveraging a dataset from the National Institute of Diabetes and Digestive and Kidney Diseases.\n",
                "\n",
                "**RESEARCH QUESTIONS**\n",
                "\n",
                "**Feature Importance**\n",
                "- Which diagnostic measures (e.g., glucose levels, BMI) are the strongest predictors of diabetes?\n",
                "- How do the relative importance of these features compare?\n",
                "\n",
                "**Feature Interactions**\n",
                "- Are there significant interactions between diagnostic measures that influence diabetes risk?\n",
                "- How do these interactions affect the predictive model?\n",
                "\n",
                "**Clinical Implications**\n",
                "- Can the model identify subgroups of patients with distinct risk profiles?\n",
                "- How can the model be used to improve clinical decision-making and early intervention?\n",
                "\n",
                "**Model Performance**\n",
                "- How well does the **`BOOSTING ALGORITHM (XGBoost)`** generalize to new, unseen data?\n",
                "- What is the impact of different hyperparameter settings on model performance?\n",
                "\n",
                "\n",
                "**Methodology**\n",
                "- **`Extreme Gradient Boosting`**\n",
                "- XGBoost, or Extreme Gradient Boosting, is a powerful machine learning algorithm that is widely used for both classification and regression tasks. It's part of a family of algorithms known as gradient boosting machines.\n",
                "\n",
                "**How does XGBoost work?**\n",
                "\n",
                "- **Sequential Model Building**:  XGBoost constructs a model sequentially. It starts by building a simple model (like a decision tree) and then adds new models one by one.\n",
                "- **Minimizing Loss**: Each new model is trained to correct the errors made by the previous models. It does this by minimizing a loss function, which measures how well the model fits the training data.\n",
                "- **Regularization**: XGBoost incorporates regularization techniques to prevent overfitting. This helps the model generalize better to unseen data.\n",
                "- **Parallel Processing**: XGBoost is designed to be highly efficient and can leverage multiple cores of a CPU or GPUs for parallel processing.\n",
                "\n",
                "\n",
                "**`XGBoost` vs. `Random Forest` vs. `Decision Tree`**\n",
                "- **Decision Tree**: A decision tree is a basic machine learning model that makes decisions by splitting the data based on certain conditions. It's a single tree-like model.\n",
                "- **Random Forest**: A random forest is an ensemble method that combines multiple decision trees. Each tree in the forest is trained on a different subset of the data and features. The final prediction is made by averaging the predictions of all the trees.\n",
                "- **XGBoost**: XGBoost is also an ensemble method, but it differs from random forest in several ways:\n",
                "    - **Sequential vs. Parallel**: Random forest builds trees independently, while XGBoost builds trees sequentially.\n",
                "    - **Optimization**: XGBoost optimizes a loss function directly, making it more efficient.\n",
                "    - **Regularization**: XGBoost incorporates regularization techniques to prevent overfitting.\n",
                "    - **Handling Missing Values**: XGBoost has built-in mechanisms for handling missing values.\n",
                "\n",
                "**To summarize:**\n",
                "- Decision trees are the building blocks of more complex models like random forests and XGBoost.\n",
                "- Random forests combine multiple decision trees to improve accuracy and reduce overfitting.\n",
                "- XGBoost is a highly optimized gradient boosting algorithm that builds models sequentially and incorporates regularization to prevent overfitting.\n",
                "\n",
                "<br>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**1.2. LIBRARY IMPORTING**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import pickle  # For saving the model\n",
                "from pickle import dump\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "from sklearn.feature_selection import f_classif, SelectKBest\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
                "from xgboost import XGBClassifier\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**1.3. DATA COLLECTION**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   Pregnancies   Glucose   Insulin       BMI  DiabetesPedigreeFunction  \\\n",
                        "0     0.176471  0.577889  0.188172  0.641414                  0.064171   \n",
                        "1     0.176471  0.567839  0.114247  0.496633                  0.488414   \n",
                        "2     0.294118  0.793970  0.282258  0.663300                  0.282531   \n",
                        "3     0.176471  0.391960  0.000000  0.547138                  0.171123   \n",
                        "4     0.000000  0.507538  0.000000  0.601010                  0.106952   \n",
                        "\n",
                        "        Age  \n",
                        "0  0.116667  \n",
                        "1  0.066667  \n",
                        "2  0.133333  \n",
                        "3  0.300000  \n",
                        "4  0.083333  \n",
                        "   Outcome\n",
                        "0        0\n",
                        "1        0\n",
                        "2        1\n",
                        "3        0\n",
                        "4        0\n"
                    ]
                }
            ],
            "source": [
                "# URLs for the processed datasets (adjust these URLs with the correct RAW paths from GitHub)\n",
                "X_train_url = \"https://raw.githubusercontent.com/jenuzho/PREDICTING-DIABETES-decision-tree/main/data/processed/X_train_without_outliers_minmax_sel.csv\"\n",
                "X_test_url = \"https://raw.githubusercontent.com/jenuzho/PREDICTING-DIABETES-decision-tree/main/data/processed/X_test_without_outliers_minmax_sel.csv\"\n",
                "y_train_url = \"https://raw.githubusercontent.com/jenuzho/PREDICTING-DIABETES-decision-tree/main/data/processed/y_train.csv\"\n",
                "y_test_url = \"https://raw.githubusercontent.com/jenuzho/PREDICTING-DIABETES-decision-tree/main/data/processed/y_test.csv\"\n",
                "\n",
                "# Load the datasets\n",
                "X_train = pd.read_csv(X_train_url)\n",
                "X_test = pd.read_csv(X_test_url)\n",
                "y_train = pd.read_csv(y_train_url)\n",
                "y_test = pd.read_csv(y_test_url)\n",
                "\n",
                "# Check the first few rows of the training data\n",
                "print(X_train.head())\n",
                "print(y_train.head())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Base Model Accuracy: 0.7727272727272727\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.84      0.79      0.81        96\n",
                        "           1       0.68      0.74      0.71        58\n",
                        "\n",
                        "    accuracy                           0.77       154\n",
                        "   macro avg       0.76      0.77      0.76       154\n",
                        "weighted avg       0.78      0.77      0.77       154\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Create the base XGBoost model\n",
                "xgb_model = XGBClassifier(random_state=42, eval_metric=\"logloss\")\n",
                "\n",
                "# Train the model on the training data\n",
                "xgb_model.fit(X_train, y_train.values.ravel())\n",
                "\n",
                "# Make predictions on the test data\n",
                "y_pred = xgb_model.predict(X_test)\n",
                "\n",
                "# Evaluate the model\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"Base Model Accuracy: {accuracy}\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Final Model Accuracy: 0.7987012987012987\n"
                    ]
                }
            ],
            "source": [
                "import xgboost as xgb\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Convert data to DMatrix (XGBoost's optimized data structure)\n",
                "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
                "dtest = xgb.DMatrix(X_test, label=y_test)\n",
                "\n",
                "# Define parameters\n",
                "param = {\n",
                "    'max_depth': 3,\n",
                "    'eta': 0.1,\n",
                "    'objective': 'binary:logistic',\n",
                "    'eval_metric': 'logloss',\n",
                "}\n",
                "\n",
                "# Perform cross-validation\n",
                "cv_results = xgb.cv(param, dtrain, num_boost_round=100, nfold=3, metrics='logloss', as_pandas=True)\n",
                "\n",
                "# Train the final model with the best number of boosting rounds\n",
                "final_model = xgb.train(param, dtrain, num_boost_round=len(cv_results))\n",
                "\n",
                "# Make predictions\n",
                "y_pred = final_model.predict(dtest)\n",
                "y_pred_binary = [1 if i > 0.5 else 0 for i in y_pred]\n",
                "\n",
                "# Evaluate the predictions\n",
                "accuracy = accuracy_score(y_test, y_pred_binary)\n",
                "print(f\"Final Model Accuracy: {accuracy}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Jen\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:52:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
                        "Parameters: { \"classifier__learning_rate\", \"classifier__max_depth\", \"classifier__n_estimators\" } are not used.\n",
                        "\n",
                        "  warnings.warn(smsg, UserWarning)\n"
                    ]
                }
            ],
            "source": [
                "from xgboost import XGBClassifier\n",
                "\n",
                "# Get the best parameters from GridSearchCV\n",
                "best_params = grid_search.best_params_\n",
                "\n",
                "# Train a new XGBClassifier with the best parameters\n",
                "best_xgb_model = XGBClassifier(**best_params, eval_metric=\"logloss\", random_state=42)\n",
                "best_xgb_model.fit(X_train, y_train.values.ravel())\n",
                "\n",
                "# Make predictions on the test set\n",
                "y_pred_optimized = best_xgb_model.predict(X_test)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Optimized Model Accuracy: 0.7077922077922078\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.68      1.00      0.81        96\n",
                        "           1       1.00      0.22      0.37        58\n",
                        "\n",
                        "    accuracy                           0.71       154\n",
                        "   macro avg       0.84      0.61      0.59       154\n",
                        "weighted avg       0.80      0.71      0.64       154\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from xgboost import XGBClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "# Clean the best parameters by removing the prefix if needed\n",
                "best_params = {key.split('__')[-1]: value for key, value in grid_search.best_params_.items()}\n",
                "\n",
                "# Train a new XGBClassifier with the cleaned parameters\n",
                "best_xgb_model = XGBClassifier(**best_params, eval_metric=\"logloss\", random_state=42)\n",
                "best_xgb_model.fit(X_train, y_train.values.ravel())\n",
                "\n",
                "# Make predictions on the test set\n",
                "y_pred_optimized = best_xgb_model.predict(X_test)\n",
                "\n",
                "# Evaluate the optimized model\n",
                "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
                "print(f\"Optimized Model Accuracy: {accuracy_optimized}\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred_optimized))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Optimized Model Accuracy: 0.7077922077922078\n",
                        "\n",
                        "Confusion Matrix:\n",
                        "[[96  0]\n",
                        " [45 13]]\n",
                        "\n",
                        "Classification Report:\n",
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.68      1.00      0.81        96\n",
                        "           1       1.00      0.22      0.37        58\n",
                        "\n",
                        "    accuracy                           0.71       154\n",
                        "   macro avg       0.84      0.61      0.59       154\n",
                        "weighted avg       0.80      0.71      0.64       154\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "# Evaluate the optimized model\n",
                "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
                "print(f\"Optimized Model Accuracy: {accuracy_optimized}\")\n",
                "\n",
                "# Confusion Matrix\n",
                "conf_matrix = confusion_matrix(y_test, y_pred_optimized)\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(conf_matrix)\n",
                "\n",
                "# Classification Report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred_optimized))\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
